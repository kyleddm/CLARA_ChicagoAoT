{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#package importing\n",
    "#from pyspark import ml\n",
    "from pyspark.sql.functions import *\n",
    "#from spark_rapids_ml.clustering import KMeans\n",
    "from cuml.cluster import HDBSCAN\n",
    "from pyspark.sql.functions import concat_ws,col,lit, row_number, monotonically_increasing_id\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql import SparkSession\n",
    "import pickle as pkl\n",
    "import pandas as pd\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import Normalizer\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "#from lenspy import DynamicPlot\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "from pyspark.sql.types import FloatType\n",
    "#from cuml import DBSCAN as dbscan\n",
    "from sklearn import preprocessing\n",
    "from numpy import int64\n",
    "#from sklearn.metrics import davies_bouldin_score\n",
    "import findspark\n",
    "#https://spark.apache.org/docs/1.2.2/ml-guide.html\n",
    "#https://pyshark.com/davies-bouldin-index-for-k-means-clustering-evaluation-in-python/\n",
    "#import torch\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!export PYSPARK_SUBMIT_ARGS=\"--master local[2] pyspark-shell\"\n",
    "!export JAVA_HOME=\"/usr/bin/java\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WEATHER_ROOT=\"/mnt/e/datasets/NOAA_GLERL/\"\n",
    "#code for combining the NOAA weather Data into a single DF\n",
    "weatherData=pd.read_csv(WEATHER_ROOT+'test2.csv')\n",
    "weatherData2019=pd.read_csv(WEATHER_ROOT+'test2019_2.csv')\n",
    "def daypad(day):\n",
    "    return(str(int64(day)).zfill(3))\n",
    "def timepad(time):\n",
    "    return(str(int64(time)).zfill(4))\n",
    "def dt_convert(year, dayOfYear,hhmm):\n",
    "    return datetime.strptime(f\"{year} {dayOfYear} {hhmm}\", '%Y %j %H%M')\n",
    "\n",
    "weatherData['UTC']=weatherData.apply(lambda x: timepad(x['UTC']),axis=1)\n",
    "weatherData['DOY']=weatherData.apply(lambda x: daypad(x['DOY']),axis=1)\n",
    "weatherData['Date']=weatherData.apply(lambda x: dt_convert(str(int64(x['Year'])), str(x['DOY']),str(x['UTC'])), axis=1)#(dt_convert(weatherData['Year'],weatherData['DOY'],weatherData['UTC']))\n",
    "\n",
    "weatherData2019['UTC']=weatherData2019.apply(lambda x: timepad(x['UTC']),axis=1)\n",
    "weatherData2019['DOY']=weatherData2019.apply(lambda x: daypad(x['DOY']),axis=1)\n",
    "weatherData2019['Date']=weatherData2019.apply(lambda x: dt_convert(str(int64(x['Year'])), str(x['DOY']),str(x['UTC'])), axis=1)\n",
    "weatherDta=weatherData.append(weatherData2019, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/10/16 09:27:20 WARN Utils: Your hostname, RogueLeader, resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)\n",
      "25/10/16 09:27:20 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/10/16 09:27:20 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "AOT_ROOT='/mnt/e/datasets/Chicago-AoT-dataset/AoT_Chicago.complete.2022-08-31/'\n",
    "OUT_ROOT='/mnt/e/outputs/chicago_aot/'\n",
    "#pull in dataset(s) and start spark session\n",
    "#with open('pre-processed/AoT_dataset_0.pkl', 'rb') as fil:\n",
    "#    pdf=pd.DataFrame(pkl.load(fil))\n",
    "#spark=SparkSession.builder.getOrCreate()\n",
    "spark = SparkSession.builder.master('local[*]').config(\"spark.driver.memory\", \"30g\").config(\"spark.driver.maxResultSize\",\"20g\").appName('Chicago_AoT').getOrCreate()\n",
    "spark.conf.set(\"spark.sql.session.timeZone\", \"UTC\")\n",
    "#spark = SparkSession.builder.config(\"spark.driver.memory\", \"20g\").getOrCreate()\n",
    "spark.conf.set('spark.rapids.sql.enabled','true')\n",
    "spark.conf.set('spark.rapids.memory.gpu.pooling.enabled','true')\n",
    "df=spark.read.csv(AOT_ROOT+'data.csv',sep=',', header=True)\n",
    "df_nodes=spark.read.csv(AOT_ROOT+'nodes.csv',sep=',', header=True)\n",
    "df_sensors=spark.read.csv(AOT_ROOT+'sensors.csv',sep=',', header=True)\n",
    "#add Lat/Lon to the main dataset\n",
    "df_new = df.withColumn('index', monotonically_increasing_id()).join(df_nodes, df.node_id == df_nodes.node_id, 'left').orderBy('index').select('timestamp', df.node_id, 'subsystem', 'sensor', 'parameter', 'value_raw', 'value_hrf', 'lat', 'lon')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 44:===================================================>(2318 + 2) / 2320]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------+---------+---------+-------------+---------+------------+---------+------------------+\n",
      "|          timestamp|     node_id|subsystem|   sensor|    parameter|value_raw|   value_hrf|      lat|               lon|\n",
      "+-------------------+------------+---------+---------+-------------+---------+------------+---------+------------------+\n",
      "|2018/01/01 00:00:06|001e0610e532|chemsense|      at0|  temperature|    -1106|      -11.06|41.857959|-87.65642700000002|\n",
      "|2018/01/01 00:00:06|001e0610e532|chemsense|      at1|  temperature|    -1077|      -10.77|41.857959|-87.65642700000002|\n",
      "|2018/01/01 00:00:06|001e0610e532|chemsense|      at2|  temperature|    -1009|      -10.09|41.857959|-87.65642700000002|\n",
      "|2018/01/01 00:00:06|001e0610e532|chemsense|      at3|  temperature|     -972|       -9.72|41.857959|-87.65642700000002|\n",
      "|2018/01/01 00:00:06|001e0610e532|chemsense|chemsense|           id|       NA|541eec3ebfa6|41.857959|-87.65642700000002|\n",
      "|2018/01/01 00:00:06|001e0610e532|chemsense|       co|concentration|     2068|          NA|41.857959|-87.65642700000002|\n",
      "|2018/01/01 00:00:06|001e0610e532|chemsense|      h2s|concentration|     -345|          NA|41.857959|-87.65642700000002|\n",
      "|2018/01/01 00:00:06|001e0610e532|chemsense|   lps25h|     pressure|   101939|     1019.39|41.857959|-87.65642700000002|\n",
      "|2018/01/01 00:00:06|001e0610e532|chemsense|   lps25h|  temperature|     -954|       -9.54|41.857959|-87.65642700000002|\n",
      "|2018/01/01 00:00:06|001e0610e532|chemsense|      no2|concentration|      649|          NA|41.857959|-87.65642700000002|\n",
      "+-------------------+------------+---------+---------+-------------+---------+------------+---------+------------------+\n",
      "only showing top 10 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_new.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_id_substring='001e061' #this is the portion of the node_id that is identical to every node\n",
    "strip_sens=['chemsense', 'metsense', 'loadavg', 'mem', 'time', 'device', 'net_rx', 'net_tx', 'ping', 'media', 'modem', 'disk_used', 'disk_size', 'disk_used_ratio', 'service_active', 'plugins', 'wagman_fc', 'wagman_cu', 'wagman_enabled', 'wagman_vdc', 'wagman_hb', 'wagman_stopping', 'wagman_starting', 'wagman_killing', 'wagman_th', 'wagman_comm', 'wagman_uptime', 'image_detector']\n",
    "sensor_name_truncation={'chemsense':'cs','alphasense':'as','metsense':'ms','plantower':'pt','audio':'aud','lightsense':'ls','wagman':'wg','microphone':'mic','image':'img'}\n",
    "#truncate all names of subsystems and node ids for space\n",
    "stripdf=df_new.filter(~col('sensor').isin(strip_sens))\n",
    "stripdf=stripdf.withColumn(\"subsystem\", when(col(\"subsystem\") == \"chemsense\", 'cs').otherwise(col(\"subsystem\")))\n",
    "stripdf=stripdf.withColumn(\"subsystem\", when(col(\"subsystem\") == \"metsense\", 'ms').otherwise(col(\"subsystem\")))\n",
    "stripdf=stripdf.withColumn(\"subsystem\", when(col(\"subsystem\") == \"lightsense\", 'ls').otherwise(col(\"subsystem\")))\n",
    "stripdf=stripdf.withColumn(\"subsystem\", when(col(\"subsystem\") == \"alphasense\", 'as').otherwise(col(\"subsystem\")))\n",
    "stripdf=stripdf.withColumn(\"subsystem\", when(col(\"subsystem\") == \"plantower\", 'pt').otherwise(col(\"subsystem\")))\n",
    "stripdf=stripdf.withColumn(\"subsystem\", when(col(\"subsystem\") == \"audio\", 'aud').otherwise(col(\"subsystem\")))\n",
    "stripdf=stripdf.withColumn(\"subsystem\", when(col(\"subsystem\") == \"wagman\", 'wg').otherwise(col(\"subsystem\")))\n",
    "stripdf=stripdf.withColumn(\"subsystem\", when(col(\"subsystem\") == \"microphone\", 'mic').otherwise(col(\"subsystem\")))\n",
    "stripdf=stripdf.withColumn(\"subsystem\", when(col(\"subsystem\") == \"image\", 'img').otherwise(col(\"subsystem\")))\n",
    "stripdf=stripdf.withColumn(\"node_id\",expr(\"substring(node_id, 8, 12)\"))\n",
    "#add UNIX timestamp instead of timestamp\n",
    "stripdf = stripdf.withColumn('unixTime',unix_timestamp(to_timestamp('timestamp','yyyy/MM/dd HH:mm:ss')))\n",
    "\n",
    "#convert data timestamps to numbers\n",
    "#stripdf=stripdf.withColumn(\"value_hrf\",stripdf[\"value_hrf\"].cast('float'))\n",
    "#stripdf=stripdf.withColumn(\"value_raw\",stripdf[\"value_raw\"].cast('int'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcUnixTime(timestamp:str):\n",
    "    from datetime import datetime\n",
    "    import pytz\n",
    "    #code from Bing 2025OCT116\n",
    "    # Define the date, time, and timezone\n",
    "    date_str = timestamp#\"2025-10-16 15:30:00\"  # Example date and time\n",
    "    timezone_str = \"UTC\"#\"America/New_York\"  # Example timezone\n",
    "    # Parse the date and time\n",
    "    dt = datetime.strptime(date_str, \"%Y/%m/%d %H:%M:%S\")\n",
    "    # Localize the datetime object to the specified timezone\n",
    "    timezone = pytz.timezone(timezone_str)\n",
    "    localized_dt = timezone.localize(dt)\n",
    "    # Convert to UNIX timestamp\n",
    "    unix_timestamp = int(localized_dt.timestamp())\n",
    "    print(\"UNIX Timestamp:\", unix_timestamp)\n",
    "    return unix_timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UNIX Timestamp: 1514764800\n"
     ]
    }
   ],
   "source": [
    "ans=calcUnixTime('2018/01/01 00:00:00')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20g\n",
      "UNIX Timestamp: 1514764800\n",
      "UNIX Timestamp: 1519862400\n"
     ]
    }
   ],
   "source": [
    "print(spark.conf.get('spark.driver.maxResultSize'))\n",
    "time_start=calcUnixTime('2018/01/01 00:00:00')\n",
    "time_end=calcUnixTime('2018/03/01 00:00:00')\n",
    "test=stripdf.filter((stripdf.unixTime >= time_start)& (stripdf.unixTime<=time_end))#.contains('2018/01/01'))\n",
    "test=test.drop('timestamp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "testdf=test.toPandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testdf.to_csv(OUT_ROOT+'aot_2018JAN-APR.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filter what we're looking for\n",
    "#result=df.filter((df.node_id=='001e0610e532') & (df.sensor=='lps25h') & (df.subsystem=='chemsense') & (df.parameter=='temperature'))\n",
    "\n",
    "#result=result.filter((result['value_hrf'] != np.nan) | (result['value_raw'] != np.nan))\n",
    "#convert date string to unixtime#no longer needed KDM2023SEP13.  There are built-in functions in pyspark to do this.  see https://stackoverflow.com/questions/77107769/pyspark-udf-function-storing-incorrect-data-despite-function-producing-correct-r\n",
    "#def timeswap(x:str):\n",
    "#    print(x)\n",
    "#    utime= datetime.timestamp(datetime.strptime(x, \"%Y/%m/%d %H:%M:%S\"))\n",
    "#    print(utime)\n",
    "#    return utime\n",
    "\n",
    "#timeUDF = spark.udf.register('timeUDF',timeswap,FloatType()) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def timeswap2(x):\n",
    "    #utime= datetime.timestamp(datetime.strptime(x, \"%Y/%m/%d %H:%M:%S\"))\n",
    "    #timeval=datetime.utcfromtimestamp(x)\n",
    "    timeval=datetime.strptime(x, \"%Y/%m/%d %H:%M:%S\")\n",
    "    #print(x)\n",
    "    #print(utime)\n",
    "    return timeval\n",
    "def configureTraces(pdf,weatherData):\n",
    "    data=[]\n",
    "    tracev=go.Scattergl(x=pdf['timestamp'], y=pdf['value_hrf'],name='001e0610e532', mode=\"markers\",marker=dict(size=3, symbol='circle-open'))\n",
    "    data.append(tracev)\n",
    "    tracep=go.Scattergl(x=pdf['timestamp'], y=pdf['prediction'] ,mode=\"markers\",name='cluster',marker=dict(size=3, color='red', symbol='circle-open'),fillcolor='red')\n",
    "    tracew=go.Scattergl(x=weatherData['Date'], y=weatherData['AirTemp(c)'] ,mode=\"markers\",name='NOAA Data',marker=dict(size=3, color='green', symbol='circle-open'),fillcolor='red')\n",
    "    data.append(tracew)\n",
    "    data.append(tracep)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#configure for time component\n",
    "assembler_t = VectorAssembler(inputCols=['unixTime','value_hrf','value_raw'], outputCol=\"features\",handleInvalid=\"keep\")\n",
    "result_t = assembler_t.transform(result)\n",
    "normalizer_t=Normalizer(inputCol='features', outputCol='normalized_features')\n",
    "result_t=normalizer_t.transform(result_t)\n",
    "pdft=result_t.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_t.select('normalized_features').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#note: it looks like spark's normalizer and pandas' min-max scaler produce different numbers  need to fix this\n",
    "def print_pandas(dataframe_given):\n",
    "    with pd.option_context('display.max_rows', None,'display.max_columns', None, 'expand_frame_repr', False, 'display.max_colwidth', None):\n",
    "        print(\"Given dataframe name\")\n",
    "        print(dataframe_given)\n",
    "print_pandas(result_t.first()['normalized_features'])\n",
    "print(x_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#configure for NO time component\n",
    "assembler_nt = VectorAssembler(inputCols=['value_hrf','value_raw'], outputCol=\"features\",handleInvalid=\"keep\")\n",
    "result_nt = assembler_nt.transform(result)\n",
    "pdfnt=result_nt.toPandas()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clara",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
